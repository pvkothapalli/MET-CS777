{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS777_Cos_pkothapalli_Assignmnet1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvkothapalli/MET-CS777/blob/master/CS777_Cos_pkothapalli_Assignmnet1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUDO2lxxGkOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initilaization section for JDK and Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://mirror.cc.columbia.edu/pub/software/apache/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!rm -rf spark-3.0.0-preview2-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KM26JrJ_G2xl",
        "colab": {}
      },
      "source": [
        "#Set up Java and Spark environmnet with spark context\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop2.7\"\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_4cvFVA7bBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#setting up Spark Context\n",
        "import findspark\n",
        "findspark.init(\"spark-3.0.0-preview2-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfyXfCLF2pCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data file ( small size -  Taxi data for 2013) from amazon s3 bucket  as a data set \n",
        "!wget https://s3.amazonaws.com/metcs777/taxi-data-sorted-small.csv.bz2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llc29iYL3LLe",
        "colab_type": "code",
        "outputId": "7845d1d3-732c-406f-c8d2-44c1406b8014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load data for  processing for all the tasks\n",
        "file = \"./taxi-data-sorted-small.csv.bz2\"\n",
        "#file = \"s3://metcs777/taxi-data-sorted-small.csv.bz2\"\n",
        "# read input text file to RDD\n",
        "lines = sc.textFile(file)\n",
        "# splt  the lines of the file with \",\" separator \n",
        "taxis = lines.map(lambda x: x.split(','))\n",
        "\n",
        "#map columns variables \n",
        "taxis_orig_data = taxis.map(lambda p: (p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7], p[8], p[9],\\\n",
        "                                  p[10], p[11], p[12], p[13], p[14], p[15], p[16]))\n",
        "\n",
        "#Provide an action to make spark rdd execute and get the count of rows\n",
        "taxis_orig_data.count()\n"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-217-a500c0758130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Provide an action to make spark rdd execute and get the count of rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtaxis_orig_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \"\"\"\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \"\"\"\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mspark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2181)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor195.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KeKgWYycsvsJ",
        "outputId": "8d3b2acf-6573-43d3-9305-19ad1f201c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Check for bad data and Filter if  Medallion and hack_license( driver id) columns or any other coulmns are null\n",
        "# refer to the Description of data for columns. 0 - Medallion, 1 - taxi id, 2 - pickup datetime, 4- trip in sec, 5 - trip distance, 12- surcharge\n",
        "# 16 - total amount\n",
        "taxis_filt_data = taxis_orig_data.filter( lambda p : False \n",
        "                                         if p[0].strip() == '' or  \\\n",
        "                                          p[1].strip() == '' or \\\n",
        "                                          p[2].strip() == '' or p[3].strip() == '' or \\\n",
        "                                          p[4].strip() == '' or p[5].strip() == '' or \\\n",
        "                                          p[6].strip() == '' or p[7].strip() == '' or \\\n",
        "                                          p[8].strip() == '' or p[9].strip() == '' or \\\n",
        "                                          p[10].strip() == '' or p[11].strip() == '' or \\\n",
        "                                          p[12].strip() == '' or p[13].strip() == '' or \\\n",
        "                                          p[14].strip() == '' or p[15].strip() == '' or \\\n",
        "                                          p[16].strip() == '' else  \\\n",
        "                                         True\n",
        "                                         )\n",
        "\n",
        "# Count of Rows\n",
        "taxis_filt_data.count()"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMoXu0DX5fUR",
        "colab_type": "code",
        "outputId": "58a293ad-c66a-40ea-cc36-4e38000ced35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#4.1 Task1  Compute Top 10 taxis Drivers using Medallion as key, generate ( medallion, no of drivers ) as pairs\n",
        "from operator import add\n",
        "# Filter Duplicate records for Medallion and Driver id as data has multiple trips by the same drivers\n",
        "driver_data_count = taxis_filt_data.map(lambda p: (p[0], p[1])).distinct()\n",
        "driver_data_count.count()"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsSRTLPwvOSs",
        "colab_type": "code",
        "outputId": "4405191e-485b-4182-c763-bfabbd01a88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from operator import add\n",
        "# Filter Duplicate records for Medallion and Driver id as data has multiple trips by the same drivers\n",
        "# Get unique Medallion and Driver ids\n",
        "driver_data_count = taxis_filt_data.map(lambda p: (p[0], p[1])).distinct()\n",
        "\n",
        "# Build a map of the Medallion with 1 which points  to a unique driver (1) and reduce by Medallion  using add\n",
        "fil_driver_data_count = driver_data_count.map(lambda p: (p[0], 1)).reduceByKey(add)\n",
        "\n",
        "#let us take 5 rows\n",
        "fil_driver_data_count.take(5)\n",
        " \n"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('9836A6780886D5D11F7BA21BAEFD0E38', 3),\n",
              " ('74DA41EDA745ED72CEC41C9C4BF3E696', 5),\n",
              " ('9406D2C34715E1DA10AD4D4DDADF4DA5', 5),\n",
              " ('D7C280941FCD886701B71BE52D350503', 2),\n",
              " ('FCC037120B4287337868BA53FD007782', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL0IaYWJq0wX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c5d724b3-3842-4d52-a13d-04181828e849"
      },
      "source": [
        "# Get the top 10 of the Medalion , Driver counts using top() function \n",
        "fil_driver_data_count.top(10, key = lambda p: p[1])\n"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('65EFB7D02BAD12D5DE757CB5D350944E', 20),\n",
              " ('3C08296D0EB7ABE24FB7328DE9B62813', 20),\n",
              " ('7DEB25123AE57111F912C0EBF92F1F63', 19),\n",
              " ('3B6AE3CF05F34ADC91DC68D20F2EB913', 19),\n",
              " ('799153A138F4E8334A1A95AE25040B83', 19),\n",
              " ('F36564AB9C6EA3B6373EB0E1680A447A', 19),\n",
              " ('55D311AD2752BC278BEF7386B25B28A9', 19),\n",
              " ('CD7B02776E69483397952DC5E1F44DFE', 19),\n",
              " ('F2A08960199BCDB7EE19411A8E7A4C5D', 18),\n",
              " ('9FB7A7C1D7B960D8B17829145C6A1CF2', 18)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZt2RyLbtqyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "af4c3a53-5886-4339-dea1-16d38dc75f85"
      },
      "source": [
        "# Get the top 10 of the Medalion , Driver counts using takeOrdered() function\n",
        "print(' 4.1 Task Top 10 Medallion Driver pairs: ')\n",
        "fil_driver_data_count.takeOrdered(10, key = lambda p: -p[1])\n"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 4.1 Task Top 10 Medallion Driver pairs: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('65EFB7D02BAD12D5DE757CB5D350944E', 20),\n",
              " ('3C08296D0EB7ABE24FB7328DE9B62813', 20),\n",
              " ('7DEB25123AE57111F912C0EBF92F1F63', 19),\n",
              " ('3B6AE3CF05F34ADC91DC68D20F2EB913', 19),\n",
              " ('799153A138F4E8334A1A95AE25040B83', 19),\n",
              " ('F36564AB9C6EA3B6373EB0E1680A447A', 19),\n",
              " ('55D311AD2752BC278BEF7386B25B28A9', 19),\n",
              " ('CD7B02776E69483397952DC5E1F44DFE', 19),\n",
              " ('F2A08960199BCDB7EE19411A8E7A4C5D', 18),\n",
              " ('9FB7A7C1D7B960D8B17829145C6A1CF2', 18)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-oGpZDp-pRW",
        "colab_type": "code",
        "outputId": "ccf8456a-e740-4f5e-e8ea-839daf618c60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 4.2 Task 2 \n",
        "# We would like to figure out who the top 10 best drivers are in terms of their average earned money per\n",
        "# minute spent carrying a customer. The total amount field is the total money earned on a trip. In the end,\n",
        "# we are interested in computing a set of (driver, money per minute) pairs.\n",
        "# average earned money per minute spent carrying a customer = (total_amount/duration of trip in secs) * 60\n",
        "# p[1], p[16]/p[4] * 60\n",
        "\n",
        "# Build a map of medallion with DriverId, TripTime and Total Paid Amount as a RDD\n",
        "\n",
        "driver_data_count = taxis_filt_data.map(lambda p: (p[1], (int(p[4]), float(p[16]))))\n",
        "\n",
        "#Get the count of Rows with multiple Rides forthe drivers\n",
        "driver_data_count.count()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5FaJygLL-P4",
        "colab_type": "code",
        "outputId": "dc090dd7-f961-4c12-8e2d-ab428d4bc225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#Take 5 row sof the data\n",
        "driver_data_count.take(5)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('E7750A37CAB07D0DFF0AF7E3573AC141', (120, 4.5)),\n",
              " ('3FF2709163DE7036FCAA4E5A3324E4BF', (0, 27.5)),\n",
              " ('778C92B26AE78A9EBDF96B49C67E4007', (120, 5.0)),\n",
              " ('BE317B986700F63C43438482792C8654', (120, 5.0)),\n",
              " ('7077F9FD5AD649AEACA4746B2537E3FA', (120, 5.0))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJLF1eBnDNPs",
        "colab_type": "code",
        "outputId": "59e0b724-1967-4641-d66d-2f825cf07d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Compute the sum of the (duration of trip time in seconds)  and sum of the (total amount by the driver)\n",
        "driver_data_count1 = taxis_filt_data.map(lambda p: (p[1], (int(p[4]), float(p[16])))).reduceByKey(lambda a,b: (a[0]+ b[0], a[1]+ b[1]))\n",
        "# Action for RDD to execute sc\n",
        "print ( 'Total 4.1 row count:', driver_data_count1.count())\n",
        "driver_data_count1.take(2)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 4.1 row count: 20354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('778C92B26AE78A9EBDF96B49C67E4007', (62220, 1377.7799999999997)),\n",
              " ('25BA06A87905667AA1FE5990E33F0E2E', (150840, 2696.18))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AECPE9BtlmOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4e9e7a00-51ed-43dc-c612-a2a04117e526"
      },
      "source": [
        "# Filter values with sum of Duration time  and sum of Total _amount as values 0\n",
        "fil_driver_data_count1 =  driver_data_count1.filter(lambda p: True if (int(p[1][0]) != 0 and  float(p[1][1]) != 0 ) else False)\n",
        "print ( 'Filtered Total 4.2 row count:', fil_driver_data_count1.count())\n",
        "fil_driver_data_count1.take(2)\n"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered Total 4.2 row count: 20338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('778C92B26AE78A9EBDF96B49C67E4007', (62220, 1377.7799999999997)),\n",
              " ('25BA06A87905667AA1FE5990E33F0E2E', (150840, 2696.18))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t23fYdZOUSWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13288567-65ed-448b-bf6c-6664364220e8"
      },
      "source": [
        "#calculate the average rate per minute by division of sums for each driver\n",
        "driver_data_count2 = fil_driver_data_count1.map(lambda x: (x[0],round(float((float(x[1][1])/int(x[1][0]))*60),2 )))\n",
        "driver_data_count2.take(2)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('778C92B26AE78A9EBDF96B49C67E4007', 1.33),\n",
              " ('25BA06A87905667AA1FE5990E33F0E2E', 1.07)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_KwuLDzjY5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8856eedc-cc73-44b0-f559-2817cebfbc76"
      },
      "source": [
        "# Get the top 10 of the average rate per minute  for the drivers\n",
        "print(' 4.2 Task Top 10   Driver Average Rate pairs: ')\n",
        "driver_data_count2.top(10, key = lambda p: round(float(p[1]),2))"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 4.2 Task Top 10   Driver Average Rate pairs: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('57D463B8F4C3382081F206E6869AA095', 3780.0),\n",
              " ('69B6FBD28F84175AB1504F6BFF001A49', 2400.0),\n",
              " ('0838C4C7DDFD9391AD66E316B5608B26', 1815.0),\n",
              " ('30B2ACBAF002305533FF0D31D34A7C06', 702.0),\n",
              " ('4C3B2A31227663A59E1AA7B45157160D', 625.0),\n",
              " ('A0AC85170C37E1D076ADE05B0238C58E', 540.0),\n",
              " ('08026D69508127F4DE855678ABCE7E0A', 375.0),\n",
              " ('D3B2DEC5DB78D91D9AFADA53B3B521B5', 330.0),\n",
              " ('6E1D7195E38AA7A36B375C1C60AD8632', 317.31),\n",
              " ('7826BDE4CE3E44EE1BB7B926A38A4B2A', 279.86)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CiYp9nmwFh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4.3. Task 3 (8 Points)\n",
        "# We are interested to know the time of the day (pickup time of day - p[2]) that has the highest profit ratio. \n",
        "# Consider the surcharge amount in dollar for each taxi ride (without tip amount) and the distance in\n",
        "# miles, and sum up the rides for each hour of the day (24 hours) â€“ consider the pickup time for your calculation.\n",
        "# Profit Ratio = (Surcharge Amount p[12] in US Dollar) / (Travel Distance p[5] in miles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxD_NEAXHF4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e93189bf-f2ae-460a-f51d-d55597d9eaa8"
      },
      "source": [
        "# filter data that has \"0.0\" dollars surcharge and 0.00 as trip distance and date has  invalid data as  zeros \n",
        "taxis_filt_data1 = taxis_filt_data.filter(lambda p: True if (p[2] != \"0000-00-00 00:00:00\" and float(p[12]) != 0.0  and   float(p[5]) != 0.0 ) else False)\n",
        "print('4.3 count of filtered2 rows:', taxis_filt_data1.count())\n",
        "# Data has multiple days and hours and Key is defined as \"Date + hour [00-24]\"\n",
        "# Map data to get date and hour as unique  Key  surcharge, trip_distance\n",
        "taxis_filt_data2 = taxis_filt_data1.map(lambda x: (x[2][0:10]+x[2][11:13], (float(x[12]), float(x[5]))) )\n",
        "#print sample 5 rows of data\n",
        "taxis_filt_data2.take(2)\n"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.3 count of filtered2 rows: 943112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2013-01-0100', (0.5, 0.44)), ('2013-01-0100', (0.5, 0.71))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCYiWsZ4LOzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1e0f6dd-c5dc-4217-8697-300e4fdb3899"
      },
      "source": [
        "#Filter  data if date format ( key)  is null or bad rows of data\n",
        "taxis_filt_data21 = taxis_filt_data2.filter(lambda p: True if (p[0] != \"0000-00-0000\" or p[0] != '' or len(p[0] != 12)) else False)\n",
        "print('4.3 count of filtered3 rows:' ,taxis_filt_data21.count())"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.3 count of filtered3 rows: 943112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBRk_8Q5dzaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "31756ea1-e7ef-40ee-8732-195f59079d6d"
      },
      "source": [
        "#compute sum of surcharge and sum of trip_distance in miles by time of day by hour ( for all the given dates and hours combination  -  defined as key ) using reduceByKey\n",
        "taxis_filt_data3 = taxis_filt_data21.reduceByKey(lambda x,y: (round(float(x[0]) + float(y[0]),2),round(float(x[1])  + float(y[1]),2)))\n",
        "print('4.3 Total number of Time of day ( keys):' , taxis_filt_data3.count())\n",
        "#Execute RDD with action function\n",
        "taxis_filt_data3.take(2)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.3 Total number of Time of day ( keys): 208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2013-01-0100', (6486.0, 36219.3)), ('2013-01-0101', (7091.0, 46674.43))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ6Kk4mUpaXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3122e412-9be0-4596-a439-936d68f55354"
      },
      "source": [
        "# compute the Profit ratio as sum(surchargeby hourof day)/sum(trip distance by hour of day)\n",
        "taxis_filt_data4 = taxis_filt_data3.map(lambda x: (x[0],  round(float(float(x[1][0])/float(x[1][1])),2))) \n",
        "print('4.3 Total number of Profit Ratio Date-time:' , taxis_filt_data4.count())\n",
        "taxis_filt_data4.take(3)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.3 Total number of Profit Ratio Date-time: 208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2013-01-0100', 0.18), ('2013-01-0101', 0.15), ('2013-01-0104', 0.14)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qQw40Xyr1-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "28f603b5-e295-40cf-e25e-eca012538de5"
      },
      "source": [
        "# Get the time of the day that has the highest profit ratio using max function\n",
        "print('4.3 Time of the day that has the highest profit ratio:' )\n",
        "taxis_filt_data4.max( lambda x: x[1])\n",
        " "
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.3 Time of the day that has the highest profit ratio:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2013-01-0411', 12.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kug4tjoys8jM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "498f349d-9f64-4969-d6f3-99803559dbc1"
      },
      "source": [
        "# Get the time of the day that has the highest profit ratio using max function using top function\n",
        "print('4.3 Time of the day that has the highest profit ratio:' )\n",
        "taxis_filt_data4.top(1, key = lambda p: p[1])"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.3 Time of the day that has the highest profit ratio:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2013-01-0411', 12.5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    }
  ]
}